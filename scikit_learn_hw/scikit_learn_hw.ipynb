{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Тема “Обучение с учителем”"
      ],
      "metadata": {
        "id": "PEmjebNL1dXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1\n",
        "Импортируйте библиотеки pandas и numpy.   \n",
        "\n",
        "Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn..  \n",
        "\n",
        "Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки\n",
        "составлял 30% от всех данных, при этом аргумент random state должен быть равен 42.  \n",
        "\n",
        "Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.\n",
        "\n",
        "Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых."
      ],
      "metadata": {
        "id": "S4iqBJ0V1daV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Загрузим данные\n",
        "boston = load_boston()\n",
        "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "y = pd.Series(boston.target)\n",
        "\n",
        "# Разделим данные на тренировочные и тестовые выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Создадим модель линейной регрессии\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Обучим модель на тренировочных данных\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Сделаем предсказание на тестовых данных\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# Выведем метрики\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HWBB7Lwi4h62",
        "outputId": "635aa77c-c8fa-4368-a884-3eea24a06251"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-de16dc7707db>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;34m<\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mwww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresearchgate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpublication\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4974606\u001b[0m\u001b[0m_Hedonic_housing_prices_and_the_demand_for_clean_air\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \"\"\")\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поскольку набор данных \"Boston House Prices\" был удален из библиотеки scikit-learn из-за этических проблем, мы будем использовать альтернативный набор данных \"California Housing dataset\", который можно загрузить с помощью функции fetch_california_housing из sklearn.datasets."
      ],
      "metadata": {
        "id": "sLVzmEMK5Kvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Загрузим данные\n",
        "california = fetch_california_housing()\n",
        "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y = pd.Series(california.target)\n",
        "\n",
        "# Разделим данные на тренировочные и тестовые выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Создадим модель линейной регрессии\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Обучим модель на тренировочных данных\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Сделаем предсказание на тестовых данных\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# Выведем метрики\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydBMD6uI5TH4",
        "outputId": "a91e3d38-bd02-4471-90b5-15d8ed9b303a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.5305677824766758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2\n",
        "Создайте модель под названием model с помощью класса RandomForestRegressor из модуля sklearn.ensemble.  \n",
        "\n",
        "Сделайте агрумент n_estimators равным 1000, max_depth должен быть равен 12 и random_state сделайте равным 42.  \n",
        "\n",
        "Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression, но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0], чтобы получить из датафрейма одномерный массив Numpy,\n",
        "так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно применение массивов вместо датафрейма.  \n",
        "\n",
        "Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания.  \n",
        "\n",
        "Напишите в комментариях к коду, какая модель в данном случае работает лучше."
      ],
      "metadata": {
        "id": "21U33lQm1ddL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Загрузим данные\n",
        "california = fetch_california_housing()\n",
        "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y = pd.Series(california.target)\n",
        "\n",
        "# Разделим данные на тренировочные и тестовые выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Создадим и обучим модель линейной регрессии\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "# Создадим модель RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n",
        "\n",
        "# Обучим модель на тренировочных данных\n",
        "model.fit(X_train, y_train.values)\n",
        "\n",
        "# Сделаем предсказание на тестовых данных\n",
        "y_pred_rf = model.predict(X_test)\n",
        "\n",
        "# Вычислим метрики\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Выведем результаты\n",
        "print(f\"Linear Regression - Mean Squared Error: {mse_lr}, R2: {r2_lr}\")\n",
        "print(f\"Random Forest Regressor - Mean Squared Error: {mse_rf}, R2: {r2_rf}\")\n",
        "\n",
        "# Сравнение моделей\n",
        "# Random Forest Regressor работает лучше, если R2 выше и MSE ниже\n",
        "if r2_rf > r2_lr:\n",
        "    print(\"Random Forest Regressor работает лучше.\")\n",
        "else:\n",
        "    print(\"Linear Regression работает лучше.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss-iRsLT54AU",
        "outputId": "0b70f303-16fd-483b-dc5e-e6cec97e3435"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression - Mean Squared Error: 0.5305677824766758, R2: 0.595770232606166\n",
            "Random Forest Regressor - Mean Squared Error: 0.26971891519368185, R2: 0.7945061536878142\n",
            "Random Forest Regressor работает лучше.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Задание 3\n",
        "Вызовите документацию для класса RandomForestRegressor, найдите информацию об атрибуте feature_importances_.  \n",
        "\n",
        "С помощью этого атрибута найдите сумму всех показателей важности, установите, какие два признака показывают наибольшую важность."
      ],
      "metadata": {
        "id": "2jQwJlVk1dhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Получим важности признаков\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Найдем сумму всех показателей важности\n",
        "total_importance = np.sum(feature_importances)\n",
        "print(f\"Сумма всех показателей важности: {total_importance}\")\n",
        "\n",
        "# Найдем два признака с наибольшей важностью\n",
        "feature_names = X.columns\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "top_features = importance_df.sort_values(by='Importance', ascending=False).head(2)\n",
        "print(\"Два признака с наибольшей важностью:\")\n",
        "print(top_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0YYSz7x65PQ",
        "outputId": "7ec5a043-8c46-4be1-9822-49059dd2e05c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сумма всех показателей важности: 0.9999999999999999\n",
            "Два признака с наибольшей важностью:\n",
            "    Feature  Importance\n",
            "0    MedInc    0.559269\n",
            "5  AveOccup    0.139126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Задание 4\n",
        "В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.Для этого датасета мы будем решать задачу классификации - будем определять,какие из транзакции по кредитной карте являются мошенническими.Данный датасет сильно несбалансирован (так как случаи мошенничества\n",
        "относительно редки),так что применение метрики accuracy не принесет пользы и не поможет выбрать лучшую модель.Мы будем вычислять AUC, то есть площадь под кривой ROC.\n",
        "\n",
        "Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.  \n",
        "\n",
        "Загрузите датасет creditcard.csv и создайте датафрейм df.  \n",
        "\n",
        "С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка несбалансирована. Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.Примените следующую настройку, чтобы можно было просматривать все столбцы датафрейма:\n",
        "pd.options.display.max_columns = 100.  \n",
        "\n",
        "Просмотрите первые 10 строк датафрейма df.  \n",
        "\n",
        "Создайте датафрейм X из датафрейма df, исключив столбец Class.  \n",
        "\n",
        "Создайте объект Series под названием y из столбца Class.  \n",
        "\n",
        "Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y.  \n",
        "У вас должны получиться объекты X_train, X_test, y_train и y_test.  \n",
        "Просмотрите информацию о их форме.  \n",
        "\n",
        "Для поиска по сетке параметров задайте такие параметры:\n",
        "parameters = [{'n_estimators': [10, 15],\n",
        "'max_features': np.arange(3, 5),\n",
        "'max_depth': np.arange(4, 7)}]  \n",
        "\n",
        "Создайте модель GridSearchCV со следующими аргументами:  \n",
        "estimator=RandomForestClassifier(random_state=100), param_grid=parameters,  \n",
        "scoring='roc_auc', cv=3.  \n",
        "\n",
        "Обучите модель на тренировочном наборе данных (может занять несколько минут).  \n",
        "\n",
        "Просмотрите параметры лучшей модели с помощью атрибута best_params_.  \n",
        "\n",
        "Предскажите вероятности классов с помощью полученной модели и метода predict_proba.  \n",
        "\n",
        "Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.  \n",
        "\n",
        "Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных, используя в качестве аргументов массивы y_test и y_pred_proba."
      ],
      "metadata": {
        "id": "53uDXRCc1dkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Загрузим датасет\n",
        "df = pd.read_csv('/content/drive/MyDrive/creditcard.csv')\n",
        "\n",
        "# Убедимся в том, что выборка несбалансирована\n",
        "print(df['Class'].value_counts(normalize=True))\n",
        "\n",
        "# Проверим, все ли столбцы содержат числовые данные и нет ли в них пропусков\n",
        "print(df.info())\n",
        "\n",
        "# Настроим отображение всех столбцов\n",
        "pd.options.display.max_columns = 100\n",
        "\n",
        "# Просмотрим первые 10 строк датафрейма\n",
        "print(df.head(10))\n",
        "\n",
        "# Создадим датафрейм X из df, исключив столбец Class\n",
        "X = df.drop('Class', axis=1)\n",
        "\n",
        "# Создадим объект Series y из столбца Class\n",
        "y = df['Class']\n",
        "\n",
        "# Разделим данные на тренировочные и тестовые наборы\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n",
        "\n",
        "# Просмотрим информацию о форме данных\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# Зададим параметры для поиска по сетке\n",
        "parameters = [{'n_estimators': [10, 15],\n",
        "               'max_features': np.arange(3, 5),\n",
        "               'max_depth': np.arange(4, 7)}]\n",
        "\n",
        "# Создадим модель GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=100),\n",
        "                           param_grid=parameters,\n",
        "                           scoring='roc_auc',\n",
        "                           cv=3)\n",
        "\n",
        "# Обучим модель на тренировочном наборе данных\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Просмотрим параметры лучшей модели\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Предскажем вероятности классов\n",
        "y_pred_proba = grid_search.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Вычислим AUC на тестовых данных\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'AUC на тестовых данных: {auc_score}')\n",
        "\n",
        "# Для сравнения вычислим AUC на тренировочных данных\n",
        "y_train_pred_proba = grid_search.predict_proba(X_train)[:, 1]\n",
        "train_auc_score = roc_auc_score(y_train, y_train_pred_proba)\n",
        "print(f'AUC на тренировочных данных: {train_auc_score}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s-gVCAm7exw",
        "outputId": "948737ee-76d0-4912-c74c-47ae03b45955"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class\n",
            "0    0.998273\n",
            "1    0.001727\n",
            "Name: proportion, dtype: float64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
            "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
            "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
            "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
            "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
            "\n",
            "         V8        V9       V10       V11       V12       V13       V14  \\\n",
            "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
            "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
            "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
            "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
            "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
            "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
            "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
            "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
            "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
            "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
            "\n",
            "        V15       V16       V17       V18       V19       V20       V21  \\\n",
            "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
            "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
            "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
            "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
            "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
            "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
            "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
            "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
            "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
            "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
            "\n",
            "        V22       V23       V24       V25       V26       V27       V28  \\\n",
            "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
            "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
            "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
            "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
            "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
            "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
            "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
            "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
            "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
            "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
            "\n",
            "   Amount  Class  \n",
            "0  149.62      0  \n",
            "1    2.69      0  \n",
            "2  378.66      0  \n",
            "3  123.50      0  \n",
            "4   69.99      0  \n",
            "5    3.67      0  \n",
            "6    4.99      0  \n",
            "7   40.80      0  \n",
            "8   93.20      0  \n",
            "9    3.68      0  \n",
            "(199364, 30) (85443, 30) (199364,) (85443,)\n",
            "{'max_depth': 6, 'max_features': 3, 'n_estimators': 15}\n",
            "AUC на тестовых данных: 0.9462664156037156\n",
            "AUC на тренировочных данных: 0.9703527882554751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Дополнительные задания:\n",
        "1). Загрузите датасет Wine из встроенных датасетов sklearn.datasets с помощью функции load_wine в переменную data.\n",
        "\n",
        "2). Полученный датасет не является датафреймом. Это структура данных, имеющая ключи аналогично словарю. Просмотрите тип данных этой структуры данных и создайте список data_keys, содержащий ее ключи.  \n",
        "\n",
        "3). Просмотрите данные, описание и названия признаков в датасете. Описание нужно вывести в виде привычного, аккуратно оформленного текста, без обозначений переноса строки, но с самими переносами и т.д."
      ],
      "metadata": {
        "id": "bguzznlX1dm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# 1. Загрузите датасет Wine\n",
        "data = load_wine()\n",
        "\n",
        "# 2. Просмотрите тип данных и создайте список data_keys\n",
        "print(type(data))  # Тип данных\n",
        "data_keys = list(data.keys())  # Ключи структуры данных\n",
        "print(data_keys)\n",
        "\n",
        "# 3. Просмотрите данные, описание и названия признаков\n",
        "# Данные\n",
        "print(data.data)\n",
        "\n",
        "# Описание\n",
        "description = data.DESCR\n",
        "print(description)\n",
        "\n",
        "# Названия признаков\n",
        "feature_names = data.feature_names\n",
        "print(feature_names)\n",
        "\n",
        "# Оформленное описание\n",
        "formatted_description = \"\"\"\n",
        "The Wine dataset is a classic and very easy multi-class classification dataset.\n",
        "It contains 178 samples of wines grown in the same region in Italy but derived from three different cultivars.\n",
        "\n",
        "**Features**:\n",
        "{}\n",
        "**Target**:\n",
        "- Class 0: {}\n",
        "- Class 1: {}\n",
        "- Class 2: {}\n",
        "\n",
        "Description:\n",
        "{}\n",
        "\"\"\".format(\", \".join(feature_names), data.target_names[0], data.target_names[1], data.target_names[2], description)\n",
        "\n",
        "print(formatted_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYhknP69Dq7",
        "outputId": "6f54f200-18d0-43a2-eabd-9a5b930b2f9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.utils._bunch.Bunch'>\n",
            "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names']\n",
            "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
            " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
            " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
            " ...\n",
            " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
            " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
            " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
            ".. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 178\n",
            "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            " \t\t- Alcohol\n",
            " \t\t- Malic acid\n",
            " \t\t- Ash\n",
            "\t\t- Alcalinity of ash  \n",
            " \t\t- Magnesium\n",
            "\t\t- Total phenols\n",
            " \t\t- Flavanoids\n",
            " \t\t- Nonflavanoid phenols\n",
            " \t\t- Proanthocyanins\n",
            "\t\t- Color intensity\n",
            " \t\t- Hue\n",
            " \t\t- OD280/OD315 of diluted wines\n",
            " \t\t- Proline\n",
            "\n",
            "    - class:\n",
            "            - class_0\n",
            "            - class_1\n",
            "            - class_2\n",
            "\t\t\n",
            "    :Summary Statistics:\n",
            "    \n",
            "    ============================= ==== ===== ======= =====\n",
            "                                   Min   Max   Mean     SD\n",
            "    ============================= ==== ===== ======= =====\n",
            "    Alcohol:                      11.0  14.8    13.0   0.8\n",
            "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "    Ash:                          1.36  3.23    2.36  0.27\n",
            "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "    Magnesium:                    70.0 162.0    99.7  14.3\n",
            "    Total Phenols:                0.98  3.88    2.29  0.63\n",
            "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "    Hue:                          0.48  1.71    0.96  0.23\n",
            "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "    Proline:                       278  1680     746   315\n",
            "    ============================= ==== ===== ======= =====\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners: \n",
            "\n",
            "Forina, M. et al, PARVUS - \n",
            "An Extendible Package for Data Exploration, Classification and Correlation. \n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science. \n",
            "\n",
            "|details-start|\n",
            "**References**\n",
            "|details-split|\n",
            "\n",
            "(1) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "Comparison of Classifiers in High Dimensional Settings, \n",
            "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
            "Mathematics and Statistics, James Cook University of North Queensland. \n",
            "(Also submitted to Technometrics). \n",
            "\n",
            "The data was used with many others for comparing various \n",
            "classifiers. The classes are separable, though only RDA \n",
            "has achieved 100% correct classification. \n",
            "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
            "(All results using the leave-one-out technique) \n",
            "\n",
            "(2) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "\"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
            "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
            "Mathematics and Statistics, James Cook University of North Queensland. \n",
            "(Also submitted to Journal of Chemometrics).\n",
            "\n",
            "|details-end|\n",
            "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
            "\n",
            "The Wine dataset is a classic and very easy multi-class classification dataset.\n",
            "It contains 178 samples of wines grown in the same region in Italy but derived from three different cultivars.\n",
            "\n",
            "**Features**:\n",
            "alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline\n",
            "**Target**:\n",
            "- Class 0: class_0\n",
            "- Class 1: class_1\n",
            "- Class 2: class_2\n",
            "\n",
            "Description:\n",
            ".. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 178\n",
            "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            " \t\t- Alcohol\n",
            " \t\t- Malic acid\n",
            " \t\t- Ash\n",
            "\t\t- Alcalinity of ash  \n",
            " \t\t- Magnesium\n",
            "\t\t- Total phenols\n",
            " \t\t- Flavanoids\n",
            " \t\t- Nonflavanoid phenols\n",
            " \t\t- Proanthocyanins\n",
            "\t\t- Color intensity\n",
            " \t\t- Hue\n",
            " \t\t- OD280/OD315 of diluted wines\n",
            " \t\t- Proline\n",
            "\n",
            "    - class:\n",
            "            - class_0\n",
            "            - class_1\n",
            "            - class_2\n",
            "\t\t\n",
            "    :Summary Statistics:\n",
            "    \n",
            "    ============================= ==== ===== ======= =====\n",
            "                                   Min   Max   Mean     SD\n",
            "    ============================= ==== ===== ======= =====\n",
            "    Alcohol:                      11.0  14.8    13.0   0.8\n",
            "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "    Ash:                          1.36  3.23    2.36  0.27\n",
            "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "    Magnesium:                    70.0 162.0    99.7  14.3\n",
            "    Total Phenols:                0.98  3.88    2.29  0.63\n",
            "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "    Hue:                          0.48  1.71    0.96  0.23\n",
            "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "    Proline:                       278  1680     746   315\n",
            "    ============================= ==== ===== ======= =====\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners: \n",
            "\n",
            "Forina, M. et al, PARVUS - \n",
            "An Extendible Package for Data Exploration, Classification and Correlation. \n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science. \n",
            "\n",
            "|details-start|\n",
            "**References**\n",
            "|details-split|\n",
            "\n",
            "(1) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "Comparison of Classifiers in High Dimensional Settings, \n",
            "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
            "Mathematics and Statistics, James Cook University of North Queensland. \n",
            "(Also submitted to Technometrics). \n",
            "\n",
            "The data was used with many others for comparing various \n",
            "classifiers. The classes are separable, though only RDA \n",
            "has achieved 100% correct classification. \n",
            "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
            "(All results using the leave-one-out technique) \n",
            "\n",
            "(2) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "\"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
            "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
            "Mathematics and Statistics, James Cook University of North Queensland. \n",
            "(Also submitted to Journal of Chemometrics).\n",
            "\n",
            "|details-end|\n",
            "\n"
          ]
        }
      ]
    }
  ]
}